{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acknowledgements\n",
    "The code in this iPython notebook used to be in R. I am very grateful to Yves Hilpisch and Michael Schwed for translating my R-code to Python.\n",
    "\n",
    "For slideshow functionality I use RISE by Damián Avila.\n",
    "\n",
    " \n",
    "Outline of presentation\n",
    "The time series of historical volatility\n",
    "\n",
    "Scaling properties\n",
    "The RFSV model\n",
    "\n",
    "Pricing under rough volatility\n",
    "\n",
    "Forecasting realized variance\n",
    "\n",
    "The time series of variance swaps\n",
    "\n",
    "Relating historical and implied\n",
    "\n",
    "The time series of realized variance\n",
    "Assuming an underlying variance process  vs\n",
    " , integrated variance  1δ∫t+δtvsds\n",
    "  may (in principle) be estimated arbitrarily accurately given enough price data.\n",
    "\n",
    "In practice, market microstructure noise makes estimation harder at very high frequency.\n",
    "Sophisticated estimators of integrated variance have been developed to adjust for market microstructure noise. See Gatheral and Oomen [6] (for example) for details of these.\n",
    "The Oxford-Man Institute of Quantitative Finance makes historical realized variance (RV) estimates freely available at http://realized.oxford-man.ox.ac.uk. These estimates are updated daily.\n",
    "\n",
    "Each day, for 21 different indices, all trades and quotes are used to estimate realized (or integrated) variance over the trading day from open to close.\n",
    "Using daily RV estimates as proxies for instantaneous variance, we may investigate the time series properties of  vt\n",
    "  empirically.\n",
    "First load all necessary Python libraries.\n",
    "\n",
    "In [1]:\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib.mlab import stineman_interp\n",
    "import pandas as pd\n",
    "import pandas.io.data as web\n",
    "import requests\n",
    "import zipfile as zi \n",
    "import StringIO as sio\n",
    "from sklearn import datasets, linear_model\n",
    "import scipy.special as scsp\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "Then update and save the latest Oxford-Man data.\n",
    "\n",
    "In [2]:\n",
    "url = 'http://realized.oxford-man.ox.ac.uk/media/1366/'\n",
    "url += 'oxfordmanrealizedvolatilityindices.zip'\n",
    "data = requests.get(url, stream=True).content\n",
    "z = zi.ZipFile(sio.StringIO(data))\n",
    "z.extractall()\n",
    "There are many different estimates of realized variance, all of them very similar. We will use the realized kernel estimates denoted by \".rk\".\n",
    "\n",
    "In [3]:\n",
    "df = pd.read_csv('OxfordManRealizedVolatilityIndices.csv', index_col=0, header=2 )\n",
    "rv1 = pd.DataFrame(index=df.index)\n",
    "for col in df.columns:\n",
    "    if col[-3:] == '.rk':\n",
    "        rv1[col] = df[col]\n",
    "rv1.index = [dt.datetime.strptime(str(date), \"%Y%m%d\") for date in rv1.index.values]\n",
    "Let's plot SPX realized variance.\n",
    "\n",
    "In [4]:\n",
    "spx = pd.DataFrame(rv1['SPX2.rk'])\n",
    "spx.plot(color='red', grid=True, title='SPX realized variance',\n",
    "         figsize=(16, 9), ylim=(0,0.003));\n",
    "\n",
    "Figure 1: Oxford-Man KRV estimates of SPX realized variance from January 2000 to the current date.\n",
    "\n",
    "In [5]:\n",
    "spx.head()\n",
    "Out[5]:\n",
    "SPX2.rk\n",
    "2000-01-03\t0.000161\n",
    "2000-01-04\t0.000264\n",
    "2000-01-05\t0.000305\n",
    "2000-01-06\t0.000149\n",
    "2000-01-07\t0.000123\n",
    "In [6]:\n",
    "spx.tail()\n",
    "Out[6]:\n",
    "SPX2.rk\n",
    "2016-04-27\t0.000031\n",
    "2016-04-28\t0.000031\n",
    "2016-04-29\t0.000048\n",
    "2016-05-02\t0.000028\n",
    "2016-05-03\t0.000041\n",
    "We can get SPX data from Yahoo using the DataReader function:\n",
    "\n",
    "In [7]:\n",
    "SPX = web.DataReader(name = '^GSPC',data_source = 'yahoo', start='2000-01-01')\n",
    "SPX = SPX['Adj Close']\n",
    "SPX.plot(title='SPX',figsize=(14, 8));\n",
    "\n",
    "The smoothness of the volatility process\n",
    "For  q≥0\n",
    " , we define the  q\n",
    " th sample moment of differences of log-volatility at a given lag  Δ\n",
    " .( ⟨⋅⟩\n",
    "  denotes the sample average):\n",
    "\n",
    "m(q,Δ)=⟨|logσt+Δ−logσt|q⟩\n",
    " \n",
    "For example\n",
    "\n",
    "m(2,Δ)=⟨(logσt+Δ−logσt)2⟩\n",
    " \n",
    "is just the sample variance of differences in log-volatility at the lag  Δ\n",
    " .\n",
    "\n",
    "Scaling of  m(q,Δ)\n",
    "  with lag  Δ\n",
    " \n",
    "In [8]:\n",
    "spx['sqrt']= np.sqrt(spx['SPX2.rk'])\n",
    "spx['log_sqrt'] = np.log(spx['sqrt'])\n",
    "\n",
    "def del_Raw(q, x): \n",
    "    return [np.mean(np.abs(spx['log_sqrt'] - spx['log_sqrt'].shift(lag)) ** q)\n",
    "            for lag in x]\n",
    "In [9]:\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('$log(\\Delta)$')\n",
    "plt.ylabel('$log\\  m(q.\\Delta)$')\n",
    "plt.ylim=(-3, -.5)\n",
    "\n",
    "zeta_q = list()\n",
    "qVec = np.array([.5, 1, 1.5, 2, 3])\n",
    "x = np.arange(1, 100)\n",
    "for q in qVec:\n",
    "    plt.plot(np.log(x), np.log(del_Raw(q, x)), 'o') \n",
    "    model = np.polyfit(np.log(x), np.log(del_Raw(q, x)), 1)\n",
    "    plt.plot(np.log(x), np.log(x) * model[0] + model[1])\n",
    "    zeta_q.append(model[0])\n",
    "    \n",
    "print zeta_q\n",
    "[0.072526605081197543, 0.14178030350291049, 0.20760692212873791, 0.27007948205423049, 0.38534332343872962]\n",
    "\n",
    "Figure 2:  logm(q,Δ)\n",
    "  as a function of  logΔ\n",
    " , SPX.\n",
    "\n",
    "Monofractal scaling result\n",
    "From the above log-log plot, we see that for each  q\n",
    " ,  m(q,Δ)∝Δζq\n",
    " .\n",
    "How does  ζq\n",
    "  scale with  q\n",
    " ?\n",
    "Scaling of  ζq\n",
    "  with  q\n",
    " \n",
    "In [10]:\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.xlabel('q')\n",
    "plt.ylabel('$\\zeta_{q}$')\n",
    "plt.plot(qVec, zeta_q, 'or')\n",
    "\n",
    "line = np.polyfit(qVec[:4], zeta_q[:4],1)\n",
    "plt.plot(qVec, line[0] * qVec + line[1])\n",
    "h_est= line[0]\n",
    "print(h_est)\n",
    "0.131697049909\n",
    "\n",
    "Figure 3: Scaling of  ζq\n",
    "  with  q\n",
    " .\n",
    "\n",
    "We find the monofractal scaling relationship\n",
    "\n",
    "ζq=qH\n",
    " \n",
    "with  H≈0.13\n",
    " .\n",
    "\n",
    "Note however that  H\n",
    "  does vary over time, in a narrow range.\n",
    "Note also that our estimate of  H\n",
    "  is biased high because we proxied instantaneous variance  vt\n",
    "  with its average over each day  1T∫T0vtdt\n",
    " , where  T\n",
    "  is one trading day.\n",
    "Estimated  H\n",
    "  for all indices\n",
    "We now repeat this analysis for all 21 indices in the Oxford-Man dataset.\n",
    "\n",
    "In [11]:\n",
    "def dlsig2(sic, x, pr=False):\n",
    "    if pr:\n",
    "        a= np.array([(sig-sig.shift(lag)).dropna() for lag in x])\n",
    "        a=a ** 2\n",
    "        print a.info()\n",
    "    return [np.mean((sig-sig.shift(lag)).dropna() ** 2) for lag in x]\n",
    "In [12]:\n",
    "h = list()\n",
    "nu = list()\n",
    "\n",
    "for col in rv1.columns:\n",
    "    sig = rv1[col]\n",
    "    sig = np.log(np.sqrt(sig))\n",
    "    sig = sig.dropna()\n",
    "    model = np.polyfit(np.log(x), np.log(dlsig2(sig, x)), 1)\n",
    "    nu.append(np.sqrt(np.exp(model[1])))\n",
    "    h.append(model[0]/2.)\n",
    "    \n",
    "OxfordH = pd.DataFrame({'names':rv1.columns, 'h_est': h, 'nu_est': nu})\n",
    "  \n",
    "In [13]:\n",
    "OxfordH\n",
    "Out[13]:\n",
    "h_est\tnames\tnu_est\n",
    "0\t0.133954\tSPX2.rk\t0.321337\n",
    "1\t0.142315\tFTSE2.rk\t0.270677\n",
    "2\t0.113366\tN2252.rk\t0.320396\n",
    "3\t0.150251\tGDAXI2.rk\t0.274873\n",
    "4\tNaN\tRUT2.rk\tNaN\n",
    "5\t0.083370\tAORD2.rk\t0.359025\n",
    "6\t0.131013\tDJI2.rk\t0.317327\n",
    "7\tNaN\tIXIC2.rk\tNaN\n",
    "8\t0.130485\tFCHI2.rk\t0.291967\n",
    "9\t0.103370\tHSI2.rk\t0.281453\n",
    "10\t0.125847\tKS11.rk\t0.274713\n",
    "11\t0.145671\tAEX.rk\t0.290187\n",
    "12\t0.178427\tSSMI.rk\t0.223249\n",
    "13\t0.128117\tIBEX2.rk\t0.281662\n",
    "14\t0.110092\tNSEI.rk\t0.324883\n",
    "15\t0.092252\tMXX.rk\t0.324180\n",
    "16\t0.106595\tBVSP.rk\t0.312907\n",
    "17\tNaN\tGSPTSE.rk\tNaN\n",
    "18\t0.119857\tSTOXX50E.rk\t0.337045\n",
    "19\t0.127094\tFTSTI.rk\t0.228910\n",
    "20\t0.133361\tFTSEMIB.rk\t0.298712\n",
    "Distributions of  (logσt+Δ−logσt)\n",
    "  for various lags  Δ\n",
    " \n",
    "Having established these beautiful scaling results for the moments, how do the histograms look?\n",
    "\n",
    "In [14]:\n",
    "def plotScaling(j, scaleFactor):\n",
    "    col_name = rv1.columns[j]\n",
    "    v = rv1[col_name]\n",
    "    x = np.arange(1,101)\n",
    "    \n",
    "    def xDel(x, lag):\n",
    "        return x-x.shift(lag)\n",
    "    \n",
    "    def sdl(lag):\n",
    "        return (xDel(np.log(v), lag)).std()\n",
    "    \n",
    "    sd1 = (xDel(np.log(v), 1)).std()\n",
    "    h = OxfordH['h_est'][j]\n",
    "    f, ax = plt.subplots(2,2,sharex=False, sharey=False, figsize=(10, 10))\n",
    "    \n",
    "    for i_0 in range(0, 2):\n",
    "        for i_1 in range(0, 2):\n",
    "            la = scaleFactor ** (i_1*1+i_0*2)\n",
    "        \n",
    "            hist_val = xDel(np.log(v), la).dropna()\n",
    "            std = hist_val.std()\n",
    "            mean = hist_val.mean()\n",
    "        \n",
    "            ax[i_0][i_1].set_title('Lag = %s Days' %la)\n",
    "            n, bins, patches = ax[i_0][i_1].hist(hist_val.values, bins=100,\n",
    "                                   normed=1, facecolor='green',alpha=0.2)\n",
    "            ax[i_0][i_1].plot(bins, mlab.normpdf(bins,mean,std), \"r\")\n",
    "            ax[i_0][i_1].plot(bins, mlab.normpdf(bins,0,sd1 * la ** h), \"b--\")\n",
    "            hist_val.plot(kind='density', ax=ax[i_0][i_1])\n",
    "        \n",
    "   \n",
    "In [15]:\n",
    " plotScaling(1,5)\n",
    "\n",
    "Figure 4: Histograms of  (logσt+Δ−logσt)\n",
    "  for various lags  Δ\n",
    " ; normal fit in red;  Δ=1\n",
    "  normal fit scaled by  Δ0.14\n",
    "  in blue.\n",
    "\n",
    "Universality?\n",
    "[Gatheral, Jaisson and Rosenbaum][5]</a></sup> compute daily realized variance estimates over one hour windows for DAX and Bund futures contracts, finding similar scaling relationships.\n",
    "We have also checked that Gold and Crude Oil futures scale similarly.\n",
    "\n",
    "Although the increments  (logσt+Δ−logσt)\n",
    "  seem to be fatter tailed than Gaussian.\n",
    "A natural model of realized volatility\n",
    "As noted originally by [Andersen et al.][1]</a></sup>, distributions of differences in the log of realized volatility are close to Gaussian.\n",
    "\n",
    "This motivates us to model  σt\n",
    "  as a lognormal random variable.\n",
    "Moreover, the scaling property of variance of RV differences suggests the model:\n",
    "(1)\n",
    "logσt+Δ−logσt=ν(WHt+Δ−WHt)\n",
    " \n",
    "where  WH\n",
    "  is fractional Brownian motion.\n",
    "\n",
    "In [Gatheral, Jaisson and Rosenbaum][5]</a></sup>, we refer to a stationary version of (1) as the RFSV (for Rough Fractional Stochastic Volatility) model.\n",
    "Fractional Brownian motion (fBm)\n",
    "Fractional Brownian motion (fBm)  {WHt;t∈R}\n",
    "  is the unique Gaussian process with mean zero and autocovariance function\n",
    "E[WHtWHs]=12{|t|2H+|s|2H−|t−s|2H}\n",
    " \n",
    "where  H∈(0,1)\n",
    "  is called the Hurst index or parameter.\n",
    "\n",
    "In particular, when  H=1/2\n",
    " , fBm is just Brownian motion.\n",
    "\n",
    "If  H>1/2\n",
    " , increments are positively correlated.% so the process is trending.\n",
    "\n",
    "If  H<1/2\n",
    " , increments are negatively correlated.% so the process is reverting.\n",
    "Representations of fBm\n",
    "There are infinitely many possible representations of fBm in terms of Brownian motion. For example, with  γ=12−H\n",
    " ,\n",
    "\n",
    "Mandelbrot-Van Ness\n",
    "\n",
    "WHt=CH{∫t−∞dWs(t−s)γ−∫0−∞dWs(−s)γ}.\n",
    " \n",
    "\n",
    "The choice\n",
    "\n",
    "CH=2HΓ(3/2−H)Γ(H+1/2)Γ(2−2H)−−−−−−−−−−−−−−−−−−−√\n",
    " \n",
    "ensures that\n",
    "\n",
    "E[WHtWHs]=12{t2H+s2H−|t−s|2H}.\n",
    " \n",
    "Does simulated RSFV data look real?\n",
    "\n",
    "Figure 8: Volatility of SPX (above) and of the RFSV model (below).\n",
    "\n",
    "Remarks on the comparison\n",
    "The simulated and actual graphs look very alike.\n",
    "\n",
    "Persistent periods of high volatility alternate with low volatility periods.\n",
    "\n",
    "H∼0.1\n",
    "  generates very rough looking sample paths (compared with  H=1/2\n",
    "  for Brownian motion).\n",
    "\n",
    "Hence rough volatility.\n",
    "\n",
    "On closer inspection, we observe fractal-type behavior.\n",
    "\n",
    "The graph of volatility over a small time period looks like the same graph over a much longer time period.\n",
    "\n",
    "This feature of volatility has been investigated both empirically and theoretically in, for example, [Bacry and Muzy][3]</a></sup> .\n",
    "\n",
    "In particular, their Multifractal Random Walk (MRW) is related to a limiting case of the RSFV model as  H→0\n",
    " .\n",
    "\n",
    "Pricing under rough volatility\n",
    "The foregoing behavior suggest the following model (see [Bayer et al.][2]</a></sup> for volatility under the real (or historical or physical) measure  P\n",
    " :\n",
    "\n",
    "logσt=νWHt.\n",
    " \n",
    "Let  γ=12−H\n",
    " . We choose the Mandelbrot-Van Ness representation of fractional Brownian motion  WH\n",
    "  as follows:\n",
    "\n",
    "WHt=CH{∫t−∞dWPs(t−s)γ−∫0−∞dWPs(−s)γ}.\n",
    " \n",
    "Then\n",
    "\n",
    "==:logvu−logvtνCH{∫ut1(u−s)γdWPs+∫t−∞[1(u−s)γ−1(t−s)γ]dWPs}2νCH[Mt(u)+Zt(u)].\n",
    " \n",
    "Note that  EP[Mt(u)|Ft]=0\n",
    "  and  Zt(u)\n",
    "  is  Ft\n",
    " -measurable.\n",
    "\n",
    "To price options, it would seem that we would need to know  Ft\n",
    " , the entire history of the Brownian motion  Ws\n",
    "  for $s\n",
    "Pricing under  P\n",
    " \n",
    "Let\n",
    "\n",
    "W~Pt(u):=2H−−−√∫utdWPs(u−s)γ\n",
    " \n",
    "With  η:=2νCH/2H−−−√\n",
    "  we have  2νCHMt(u)=ηW~Pt(u)\n",
    "  so denoting the stochastic exponential by  E(⋅)\n",
    " , we may write\n",
    "\n",
    "vu==vtexp{ηW~Pt(u)+2νCHZt(u)}EP[vu|Ft]E(ηW~Pt(u)).\n",
    " \n",
    "The conditional distribution of  vu\n",
    "  depends on  Ft\n",
    "  only through the variance forecasts  EP[vu|Ft]\n",
    " ,\n",
    "To price options, one does not need to know  Ft\n",
    " , the entire history of the Brownian motion  WPs\n",
    "  for $s\n",
    "Pricing under  Q\n",
    " \n",
    "Our model under  P\n",
    "  reads:\n",
    "\n",
    "(2)\n",
    "vu=EP[vu|Ft]E(ηW~Pt(u)).\n",
    " \n",
    "Consider some general change of measure\n",
    "\n",
    "dWPs=dWQs+λsds,\n",
    " \n",
    "where  {λs:s>t}\n",
    "  has a natural interpretation as the price of volatility risk.\n",
    "\n",
    "We may then rewrite (2) as\n",
    "\n",
    "vu=EP[vu|Ft]E(ηW~Qt(u))exp{η2H−−−√∫utλs(u−s)γds}.\n",
    " \n",
    "Although the conditional distribution of  vu\n",
    "  under  P\n",
    "  is lognormal, it will not be lognormal in general under  Q\n",
    " .\n",
    "\n",
    "The upward sloping smile in VIX options means  λs\n",
    "  cannot be deterministic in this picture.\n",
    "The rough Bergomi (rBergomi) model\n",
    "Let's nevertheless consider the simplest change of measure\n",
    "\n",
    "dWPs=dWQs+λ(s)ds,\n",
    " \n",
    "where  λ(s)\n",
    "  is a deterministic function of  s\n",
    " . Then from (2), we would have\n",
    "\n",
    "vu==EP[vu|Ft]E(ηW~Qt(u))exp{η2H−−−√∫ut1(u−s)γλ(s)ds}ξt(u)E(ηW~Qt(u))\n",
    " \n",
    "where the forward variances  ξt(u)=EQ[vu|Ft]\n",
    "  are (at least in principle) tradable and observed in the market.\n",
    "\n",
    "ξt(u)\n",
    "  is the product of two terms:\n",
    "\n",
    "EP[vu|Ft]\n",
    "  which depends on the historical path $\\{W_s, s\n",
    "a term which depends on the price of risk  λ(s)\n",
    " .\n",
    "Features of the rough Bergomi model\n",
    "The rBergomi model is a non-Markovian generalization of the Bergomi model:\n",
    "E[vu|Ft]≠E[vu|vt].\n",
    " \n",
    "The rBergomi model is Markovian in the (infinite-dimensional) state vector  EQ[vu|Ft]=ξt(u)\n",
    " .\n",
    "We have achieved our aim from Session 1 of replacing the exponential kernels in the Bergomi model with a power-law kernel.\n",
    "We may therefore expect that the rBergomi model will generate a realistic term structure of ATM volatility skew.\n",
    "Re-interpretation of the conventional Bergomi model\n",
    "A conventional  n\n",
    " -factor Bergomi model is not self-consistent for an arbitrary choice of the initial forward variance curve  ξt(u)\n",
    " .\n",
    "\n",
    "ξt(u)=E[vu|Ft]\n",
    "  should be consistent with the assumed dynamics.\n",
    "Viewed from the perspective of the fractional Bergomi model however:\n",
    "\n",
    "The initial curve  ξt(u)\n",
    "  reflects the history $\\{W_s; s\n",
    "The exponential kernels in the exponent of the conventional Bergomi model approximate more realistic power-law kernels.\n",
    "The conventional two-factor Bergomi model is then justified in practice as a tractable Markovian engineering approximation to a more realistic fractional Bergomi model.\n",
    "The stock price process\n",
    "The observed anticorrelation between price moves and volatility moves may be modeled naturally by anticorrelating the Brownian motion  W\n",
    "  that drives the volatility process with the Brownian motion driving the price process.\n",
    "Thus\n",
    "dStSt=vt−−√dZt\n",
    " \n",
    "with\n",
    "dZt=ρdWt+1−ρ2−−−−−√dW⊥t\n",
    " \n",
    "where  ρ\n",
    "  is the correlation between volatility moves and price moves.\n",
    "Simulation of the rBergomi model\n",
    "We simulate the rBergomi model as follows:\n",
    "\n",
    "Construct the joint covariance matrix for the Volterra process  W~\n",
    "  and the Brownian motion  Z\n",
    "  and compute its Cholesky decomposition.\n",
    "For each time, generate iid normal random vectors {and multiply them by the lower-triangular matrix obtained by the Cholesky decomposition} to get a  m×2n\n",
    "  matrix of paths of  W~\n",
    "  and  Z\n",
    "  with the correct joint marginals.\n",
    "With these paths held in memory, we may evaluate the expectation under  Q\n",
    "  of any payoff of interest.\n",
    "This procedure is very slow!\n",
    "\n",
    "Speeding up the simulation is work in progress.\n",
    "Guessing rBergomi model parameters\n",
    "The rBergomi model has only three parameters:  H\n",
    " ,  η\n",
    "  and  ρ\n",
    " .\n",
    "If we had a fast simulation, we could just iterate on these parameters to find the best fit to observed option prices. But we don't.\n",
    "However, the model parameters  H\n",
    " ,  η\n",
    "  and  ρ\n",
    "  have very direct interpretations:\n",
    "\n",
    "H\n",
    "  controls the decay of ATM skew  ψ(τ)\n",
    "  for very short expirations.\n",
    "\n",
    "The product  ρη\n",
    "  sets the level of the ATM skew for longer expirations.\n",
    "\n",
    "Keeping  ρη\n",
    "  constant but decreasing  ρ\n",
    "  (so as to make it more negative) pushes the minimum of each smile towards higher strikes.\n",
    "So we can guess parameters in practice.\n",
    "As we will see, even without proper calibration (i.e. just guessing parameters), rBergomi model fits to the volatility surface are amazingly good.\n",
    "SPX smiles in the rBergomi model\n",
    "In Figures 9 and 10, we show how well a rBergomi model simulation with guessed parameters fits the SPX option market as of February 4, 2010, a day when the ATM volatility term structure happened to be pretty flat.\n",
    "\n",
    "rBergomi parameters were:  H=0.07\n",
    " ,  η=1.9\n",
    " ,  ρ=−0.9\n",
    " .\n",
    "Only three parameters to get a very good fit to the whole SPX volatility surface!\n",
    "rBergomi fits to SPX smiles as of 04-Feb-2010\n",
    "\n",
    "Figure 9: Red and blue points represent bid and offer SPX implied volatilities; orange smiles are from the rBergomi simulation.\n",
    "\n",
    "Shortest dated smile as of February 4, 2010\n",
    "\n",
    "Figure 10: Red and blue points represent bid and offer SPX implied volatilities; orange smile is from the rBergomi simulation.\n",
    "\n",
    "ATM volatilities and skews\n",
    "In Figures 11 and 12, we see just how well the rBergomi model can match empirical skews and vols. Recall also that the parameters we used are just guesses!\n",
    "\n",
    "Term structure of ATM skew as of February 4, 2010\n",
    "\n",
    "Figure 11: Blue points are empirical skews; the red line is from the rBergomi simulation.\n",
    "\n",
    "Term structure of ATM vol as of February 4, 2010\n",
    "\n",
    "Figure 12: Blue points are empirical ATM volatilities; the red line is from the rBergomi simulation.\n",
    "\n",
    "Another date\n",
    "Now we take a look at another date: August 14, 2013, two days before the last expiration date in our dataset.\n",
    "Options set at the open of August 16, 2013 so only one trading day left.\n",
    "Note in particular that the extreme short-dated smile is well reproduced by the rBergomi model.\n",
    "There is no need to add jumps!\n",
    "SPX smiles as of August 14, 2013\n",
    "\n",
    "Figure 13: Red and blue points represent bid and offer SPX implied volatilities; orange smiles are from the rBergomi simulation.\n",
    "\n",
    "The forecast formula\n",
    "In the RFSV model (1),  logσt≈νWHt+C\n",
    "  for some constant  C\n",
    " .\n",
    "[Nuzman and Poor][7]</a></sup> show that  WHt+Δ\n",
    "  is conditionally Gaussian with conditional expectation\n",
    "E[WHt+Δ|Ft]=cos(Hπ)πΔH+1/2∫t−∞WHs(t−s+Δ)(t−s)H+1/2ds\n",
    " \n",
    "and conditional variance\n",
    "\n",
    "Var[WHt+Δ|Ft]=cΔ2H.\n",
    " \n",
    "where\n",
    "c=Γ(3/2−H)Γ(H+1/2)Γ(2−2H).\n",
    " \n",
    "The forecast formula\n",
    "Thus, we obtain\n",
    "\n",
    "Variance forecast formula\n",
    "\n",
    "(3)\n",
    "EP[vt+Δ|Ft]=exp{EP[log(vt+Δ)|Ft]+2cν2Δ2H}\n",
    " \n",
    "\n",
    "where\n",
    "\n",
    "EP[logvt+Δ|Ft]=cos(Hπ)πΔH+1/2∫t−∞logvs(t−s+Δ)(t−s)H+1/2ds.\n",
    " \n",
    "Implement variance forecast in Python\n",
    "In [16]:\n",
    "def c_tilde(h):\n",
    "    return scsp.gamma(3. / 2. - h) / scsp.gamma(h + 1. / 2.) * scsp.gamma(2. - 2. * h)\n",
    "\n",
    "def forecast_XTS(rvdata, h, date, nLags, delta, nu):\n",
    "    i = np.arange(nLags)\n",
    "    cf = 1./((i + 1. / 2.) ** (h + 1. / 2.) * (i + 1. / 2. + delta))\n",
    "    ldata = rvdata.truncate(after=date)\n",
    "    l = len(ldata)\n",
    "    ldata = np.log(ldata.iloc[l - nLags:])\n",
    "    ldata['cf'] = np.fliplr([cf])[0]\n",
    "    # print ldata\n",
    "    ldata = ldata.dropna()\n",
    "    fcst = (ldata.iloc[:, 0] * ldata['cf']).sum() / sum(ldata['cf'])\n",
    "    \n",
    "    return math.exp(fcst + 2 * nu ** 2 * c_tilde(h) * delta ** (2 * h))\n",
    "SPX actual vs forecast variance\n",
    "In [17]:\n",
    "rvdata = pd.DataFrame(rv1['SPX2.rk'])\n",
    "nu  = OxfordH['nu_est'][0] # Vol of vol estimate for SPX\n",
    "h = OxfordH['h_est'][0] \n",
    "n = len(rvdata)\n",
    "delta = 1\n",
    "nLags = 500\n",
    "dates = rvdata.iloc[nLags:n-delta].index\n",
    "rv_predict = [forecast_XTS(rvdata, h=h, date=d, nLags=nLags,\n",
    "                           delta=delta, nu=nu) for d in dates]\n",
    "rv_actual = rvdata.iloc[nLags+delta:n].values\n",
    "Scatter plot of delta days ahead predictions\n",
    "In [18]:\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(rv_predict, rv_actual, 'bo');\n",
    "\n",
    "Figure 14: Actual vols vs predicted vols.\n",
    "\n",
    "Superimpose actual and predicted vols\n",
    "In [19]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "vol_actual = np.sqrt(np.multiply(rv_actual,252))\n",
    "vol_predict = np.sqrt(np.multiply(rv_predict,252))\n",
    "plt.plot(vol_actual, \"b\")\n",
    "plt.plot(vol_predict, \"r\");\n",
    "\n",
    "Figure 15: Actual volatilities in blue; predicted vols in red.\n",
    "\n",
    "Forecasting the variance swap curve\n",
    "Finally, we forecast the whole variance swap curve using the variance forecasting formula (3).\n",
    "\n",
    "In [20]:\n",
    "def xi(date, tt, nu,h, tscale):  # dt=(u-t) is in units of years\n",
    "    rvdata = pd.DataFrame(rv1['SPX2.rk'])\n",
    "    return [ forecast_XTS(rvdata,h=h,date=date,nLags=500,delta=dt*tscale,nu=nu) for dt in tt]\n",
    "\n",
    "nu = OxfordH[\"nu_est\"][0]\n",
    "h = OxfordH[\"h_est\"][0]\n",
    "\n",
    "def varSwapCurve(date, bigT, nSteps, nu, h, tscale, onFactor):\n",
    "  # Make vector of fwd variances\n",
    "  tt = [ float(i) * (bigT) / nSteps for i in range(nSteps+1)]\n",
    "  delta_t = tt[1]\n",
    "  xicurve = xi(date, tt, nu, h, tscale)\n",
    "  xicurve_mid = (np.array(xicurve[0:nSteps]) + np.array(xicurve[1:nSteps+1])) / 2\n",
    "  xicurve_int = np.cumsum(xicurve_mid) * delta_t\n",
    "  varcurve1 = np.divide(xicurve_int, np.array(tt[1:]))\n",
    "  varcurve = np.array([xicurve[0],]+list(varcurve1))\n",
    "  varcurve = varcurve * onFactor * tscale #  onFactor is to compensate for overnight moves\n",
    "  res = pd.DataFrame({\"texp\":np.array(tt), \"vsQuote\":np.sqrt(varcurve)})\n",
    "  return(res)\n",
    "In [21]:\n",
    "def varSwapForecast(date,tau,nu,h,tscale,onFactor):\n",
    "  vsc = varSwapCurve(date, bigT=2.5, nSteps=100, nu=nu, h=h,\n",
    "                    tscale=tscale, onFactor=onFactor) # Creates the whole curve\n",
    "  x = vsc['texp']\n",
    "  y = vsc['vsQuote']\n",
    "  res = stineman_interp(tau,x,y,None)\n",
    "\n",
    "  return(res)\n",
    "\n",
    "# Test the function\n",
    "\n",
    "tau = (.25,.5,1,2)\n",
    "date = dt.datetime(2008,9,8)\n",
    "varSwapForecast(date,tau,nu=nu,h=h,tscale=252,onFactor=1)\n",
    "Out[21]:\n",
    "array([ 0.21949454,  0.21398188,  0.2117466 ,  0.21262899])\n",
    "'Constructing a time series of variance swap curves\n",
    "For each of 2,658 days from Jan 27, 2003 to August 31, 2013:\n",
    "\n",
    "We compute proxy variance swaps from closing prices of SPX options sourced from OptionMetrics (www.optionmetrics.com) via WRDS.\n",
    "We form the forecasts  EP[vu|Ft]\n",
    "  using (3) with 500 lags of SPX RV data sourced from The Oxford-Man Institute of Quantitative Finance (http://realized.oxford-man.ox.ac.uk).\n",
    "We note that the actual variance swap curve is a factor (of roughly 1.4) higher than the forecast, which we may attribute to a combination of overnight movements of the index and the price of volatility risk.\n",
    "Forecasts must therefore be rescaled to obtain close-to-close realized variance forecasts.\n",
    "3-month forecast vs actual variance swaps\n",
    "\n",
    "Figure 16: Actual (proxy) 3-month variance swap quotes in blue vs forecast in red (with no scaling factor).\n",
    "\n",
    "Ratio of actual to forecast\n",
    "\n",
    "Figure 17: The ratio between 3-month actual variance swap quotes and 3-month forecasts.\n",
    "\n",
    "The Lehman weekend\n",
    "Empirically, it seems that the variance curve is a simple scaling factor times the forecast, but that this scaling factor is time-varying.\n",
    "\n",
    "We can think of this factor as having two multiplicative components: the overnight factor, and the price of volatility risk.\n",
    "Recall that as of the close on Friday September 12, 2008, it was widely believed that Lehman Brothers would be rescued over the weekend. By Monday morning, we knew that Lehman had failed.\n",
    "In Figure 18, we see that variance swap curves just before and just after the collapse of Lehman are just rescaled versions of the RFSV forecast curves.\n",
    "We need variance swap estimates for 12-Sep-2008 and 15-Sep-2008\n",
    "We proxy these by taking SVI fits for the two dates and computing the log-strips.\n",
    "\n",
    "In [22]:\n",
    "varSwaps12 =(\n",
    "    0.2872021, 0.2754535, 0.2601864, 0.2544684, 0.2513854, 0.2515314,\n",
    "    0.2508418, 0.2520099, 0.2502763, 0.2503309, 0.2580933, 0.2588361, \n",
    "    0.2565093)\n",
    "\n",
    "texp12 = (\n",
    "    0.01916496, 0.04654346, 0.09582478, 0.19164956, 0.26830938, 0.29842574,\n",
    "    0.51745380, 0.54483231, 0.76659822, 0.79397673, 1.26488706, 1.76317591, \n",
    "    2.26146475)\n",
    "\n",
    "varSwaps15 = (\n",
    "    0.4410505, 0.3485560, 0.3083603, 0.2944378, 0.2756881, 0.2747838, \n",
    "    0.2682212, 0.2679770, 0.2668113, 0.2706713, 0.2729533, 0.2689598, \n",
    "    0.2733176)\n",
    "\n",
    "texp15 = (\n",
    "    0.01095140, 0.03832991, 0.08761123, 0.18343600, 0.26009582, 0.29021218, \n",
    "    0.50924025, 0.53661875, 0.75838467, 0.78576318, 1.25667351, 1.75496235, \n",
    "    2.25325120)\n",
    "Actual vs predicted over the Lehman weekend\n",
    "In [23]:\n",
    "nu = OxfordH['nu_est'][0]\n",
    "h = OxfordH['h_est'][0]\n",
    "date1 = dt.datetime(2008, 9, 12)\n",
    "date2 = dt.datetime(2008, 9, 15)\n",
    "\n",
    "# Variance curve fV model forecasts\n",
    "tau1000 = [ float(i) * 2.5 / 1000. for i in range(1,1001)]\n",
    "vs1 = varSwapForecast(date1, tau1000, nu=nu,h=h, tscale=252, onFactor=1.29)\n",
    "vs2 = varSwapForecast(date2, tau1000, nu=nu,h=h, tscale=252, onFactor=1.29)\n",
    "In [24]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.plot(texp12, varSwaps12, \"r\")\n",
    "plt.plot(texp15, varSwaps15, \"b\")\n",
    "plt.plot(tau1000, vs1, \"r--\")\n",
    "plt.plot(tau1000, vs2, \"b--\");\n",
    "\n",
    "Figure 18: SPX variance swap curves as of September 12, 2008 (red) and September 15, 2008 (blue). The dashed curves are RFSV model forecasts rescaled by the 3-month ratio ( 1.29\n",
    " ) as of the Friday close.\n",
    "\n",
    "Remarks\n",
    "We note that\n",
    "\n",
    "The actual variance swaps curves are very close to the forecast curves, up to a scaling factor.\n",
    "We are able to explain the change in the variance swap curve with only one extra observation: daily variance over the trading day on Monday 15-Sep-2008.\n",
    "The SPX options market appears to be backward-looking in a very sophisticated way.\n",
    "The Flash Crash\n",
    "The so-called Flash Crash of Thursday May 6, 2010 caused intraday realized variance to be much higher than normal.\n",
    "In Figure 19, we plot the actual variance swap curves as of the Wednesday and Friday market closes together with forecast curves rescaled by the 3-month ratio as of the close on Wednesday May 5 (which was  2.52\n",
    " ).\n",
    "We see that the actual variance curve as of the close on Friday is consistent with a forecast from the time series of realized variance that includes the anomalous price action of Thursday May 6.\n",
    "Variance swap estimates\n",
    "We again proxy variance swaps for 05-May-2010, 07-May-2010 and 10-May-2010 by taking SVI fits (see [Gatheral and Jacquier][4]</a></sup> ) for the three dates and computing the log-strips.\n",
    "\n",
    "In [25]:\n",
    "varSwaps5 = (\n",
    "    0.4250369, 0.2552473, 0.2492892, 0.2564899, 0.2612677, 0.2659618, 0.2705928, 0.2761203,\n",
    "    0.2828139, 0.2841165, 0.2884955, 0.2895839, 0.2927817, 0.2992602, 0.3116500)\n",
    "\n",
    "texp5 = (\n",
    "    0.002737851, 0.043805613, 0.120465435, 0.150581793, 0.197125257, 0.292950034,\n",
    "    0.369609856, 0.402464066, 0.618754278, 0.654346338, 0.867898700, 0.900752909,\n",
    "    1.117043121, 1.615331964, 2.631074606)\n",
    " \n",
    "varSwaps7 = (\n",
    "    0.5469727, 0.4641713, 0.3963352, 0.3888213, 0.3762354, 0.3666858, 0.3615814, 0.3627013,\n",
    "    0.3563324, 0.3573946, 0.3495730, 0.3533829, 0.3521515, 0.3506186, 0.3594066)\n",
    "\n",
    "texp7 = (\n",
    "    0.01642710, 0.03832991, 0.11498973, 0.14510609, 0.19164956, 0.28747433, 0.36413415,\n",
    "    0.39698836, 0.61327858, 0.64887064, 0.86242300, 0.89527721, 1.11156742, 1.60985626,\n",
    "    2.62559890)\n",
    "\n",
    "varSwaps10 = (\n",
    "    0.3718439, 0.3023223, 0.2844810, 0.2869835, 0.2886912, 0.2905637, 0.2957070, 0.2960737,\n",
    "    0.3005086, 0.3031188, 0.3058492, 0.3065815, 0.3072041, 0.3122905, 0.3299425)\n",
    "\n",
    "texp10 = (\n",
    "    0.008213552, 0.030116359, 0.106776181, 0.136892539, 0.183436003, 0.279260780,\n",
    "    0.355920602, 0.388774812, 0.605065024, 0.640657084, 0.854209446, 0.887063655,\n",
    "    1.103353867, 1.601642710, 2.617385352)\n",
    "In [26]:\n",
    "date1 = dt.datetime(2010, 5, 5)\n",
    "date2 = dt.datetime(2010, 5, 7)\n",
    "\n",
    "vsf5 = varSwapCurve(date1, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "vsf7 = varSwapCurve(date2, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "In [27]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.plot(texp5, varSwaps5, \"r\", label='May 5')\n",
    "plt.plot(texp7, varSwaps7, \"g\", label='May 7')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time to maturity\")\n",
    "plt.ylabel(\"Variance swap quote\")\n",
    "\n",
    "plt.plot(vsf5['texp'], vsf5['vsQuote'], \"r--\")\n",
    "plt.plot(vsf7['texp'], vsf7['vsQuote'], \"g--\");\n",
    "\n",
    "Figure 19: SPX variance swap curves as of May 5, 2010 (red) and May 7, 2010 (green). The dashed curves are RFSV model forecasts rescaled by the 3-month ratio ( 2.52\n",
    " ) as of the close on Wednesday May 5. The curve as of the close on May 7 is consistent with the forecast including the crazy moves on May 6.\n",
    "\n",
    "The weekend after the Flash Crash\n",
    "Now we plot forecast and actual variance swap curves as of the close on Friday May 7 and Monday May 10.\n",
    "\n",
    "In [28]:\n",
    "date1 = dt.datetime(2010,5,7)\n",
    "date2 = dt.datetime(2010,5,10)\n",
    "\n",
    "vsf7 = varSwapCurve(date1, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "vsf10 = varSwapCurve(date2, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "In [29]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.plot(texp7, varSwaps7, \"g\", label='May 7')\n",
    "plt.plot(texp10, varSwaps10, \"m\", label='May 10')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time to maturity\")\n",
    "plt.ylabel(\"Variance swap quote\")\n",
    "\n",
    "plt.plot(vsf7['texp'], vsf7['vsQuote'], \"g--\")\n",
    "plt.plot(vsf10['texp'], vsf10['vsQuote'], \"m--\");\n",
    "\n",
    "Figure 20: The May 10 actual curve is inconsistent with a forecast that includes the Flash Crash.\n",
    "\n",
    "Now let's see what happens if we exclude the Flash Crash from the time series used to generate the variance curve forecast.\n",
    "\n",
    "In [30]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "ax = plt.subplot(111)\n",
    "rvdata_p = rvdata.drop((dt.datetime(2010, 5, 6)), axis=0)\n",
    "rvdata.loc[\"2010-05-04\":\"2010-05-10\"].plot(ax=ax, legend=False)\n",
    "rvdata_p.loc[\"2010-05-04\":\"2010-05-10\"].plot(ax=ax, legend=False);\n",
    "\n",
    "Figure 21: rvdata_p has the May 6 realized variance datapoint eliminated (green line). Notice the crazy realized variance estimate for May 6!\n",
    "\n",
    "We need a new variance curve forecast function that uses the new time series.\n",
    "\n",
    "In [31]:\n",
    "def xip(date, tt, nu,h, tscale):  # dt=(u-t) is in units of years\n",
    "    rvdata = pd.DataFrame(rv1['SPX2.rk'])\n",
    "    rvdata_p = rvdata.drop((dt.datetime(2010, 5, 6)), axis=0)\n",
    "    return [ forecast_XTS(rvdata_p, h=h, date=date,nLags=500,\n",
    "                          delta=delta_t * tscale, nu=nu) for delta_t in tt]\n",
    "\n",
    "nu = OxfordH[\"nu_est\"][0]\n",
    "h = OxfordH[\"h_est\"][0]\n",
    "\n",
    "def varSwapCurve_p(date, bigT, nSteps, nu, h, tscale, onFactor):\n",
    "  # Make vector of fwd variances\n",
    "  tt = [ float(i) * (bigT) / nSteps for i in range(nSteps+1)]\n",
    "  delta_t = tt[1]\n",
    "  xicurve = xip(date, tt, nu, h, tscale)\n",
    "  xicurve_mid = (np.array(xicurve[0:nSteps]) + np.array(xicurve[1:nSteps + 1])) / 2\n",
    "  xicurve_int = np.cumsum(xicurve_mid) * delta_t\n",
    "  varcurve1 = np.divide(xicurve_int, np.array(tt[1:]))\n",
    "  varcurve = np.array([xicurve[0],]+list(varcurve1))\n",
    "  varcurve = varcurve * onFactor * tscale #  onFactor is to compensate for overnight moves\n",
    "  res = pd.DataFrame({\"texp\":np.array(tt), \"vsQuote\":np.sqrt(varcurve)})\n",
    "  return(res)\n",
    "\n",
    "def varSwapForecast_p(date, tau, nu, h, tscale, onFactor):\n",
    "  vsc = varSwapCurve_p(date, bigT=2.5, nSteps=100, nu=nu, h=h,\n",
    "                    tscale=tscale, onFactor=onFactor) # Creates the whole curve\n",
    "  x = vsc['texp']\n",
    "  y = vsc['vsQuote']\n",
    "  res = stineman_interp(tau, x, y, None)\n",
    "\n",
    "  return(res)\n",
    "\n",
    "# Test the function\n",
    "\n",
    "tau = (.25, .5 ,1, 2)\n",
    "date = dt.datetime(2010, 5, 10)\n",
    "varSwapForecast_p(date, tau, nu=nu, h=h, tscale=252, onFactor=1. / (1 - .35))\n",
    "Out[31]:\n",
    "array([ 0.26077084,  0.25255902,  0.25299844,  0.26116175])\n",
    "Finally, we compare our new forecast curves with the actuals.\n",
    "\n",
    "In [32]:\n",
    "date1 = dt.datetime(2010, 5, 7)\n",
    "date2 = dt.datetime(2010, 5, 10)\n",
    "\n",
    "vsf7 = varSwapCurve(date1, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "vsf10p = varSwapCurve_p(date2, bigT=2.5, nSteps=100, nu=nu, h=h, tscale=252, onFactor=2.52)\n",
    "In [33]:\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.plot(texp7, varSwaps7, \"g\", label='May 7')\n",
    "plt.plot(texp10, varSwaps10, \"m\", label='May 10')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time to maturity\")\n",
    "plt.ylabel(\"Variance swap quote\")\n",
    "\n",
    "plt.plot(vsf7['texp'], vsf7['vsQuote'], \"g--\")\n",
    "plt.plot(vsf10p['texp'], vsf10p['vsQuote'], \"m--\");\n",
    "\n",
    "Figure 22: The May 10 actual curve is consistent with a forecast that excludes the Flash Crash.\n",
    "\n",
    "Resetting of expectations over the weekend\n",
    "In Figures 20 and 22, we see that the actual variance swap curve on Monday, May 10 is consistent with a forecast that excludes the Flash Crash.\n",
    "Volatility traders realized that the Flash Crash should not influence future realized variance projections.\n",
    "Summary\n",
    "We uncovered a remarkable monofractal scaling relationship in historical volatility.\n",
    "\n",
    "A corollary is that volatility is not a long memory process, as widely believed.\n",
    "This leads to a natural non-Markovian stochastic volatility model under  P\n",
    " .\n",
    "The simplest specification of  dQdP\n",
    "  gives a non-Markovian generalization of the Bergomi model.\n",
    "The history of the Brownian motion $\\lbrace W_s, s\n",
    "This model fits the observed volatility surface surprisingly well with very few parameters.\n",
    "For perhaps the first time, we have a simple consistent model of historical and implied volatility.\n",
    "References\n",
    "\n",
    "\n",
    "^ Torben G Andersen, Tim Bollerslev, Francis X Diebold, and Heiko Ebens, The distribution of realized stock return volatility, *Journal of Financial Economics* **61**(1) 43-76 (2001).\n",
    "^ Christian Bayer, Peter Friz and Jim Gatheral, Pricing under rough volatility, *Quantitative Finance* forthcoming, available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554754, (2015).\n",
    "^ Emmanuel Bacry and Jean-François Muzy, Log-infinitely divisible multifractal processes, *Communications in Mathematical Physics* **236**(3) 449-475 (2003).\n",
    "^ Jim Gatheral and Antoine Jacquier, Arbitrage-free SVI volatility surfaces, *Quantitative Finance* **14**(1) 59-71 (2014).\n",
    "^ Jim Gatheral, Thibault Jaisson and Mathieu Rosenbaum, Volatility is rough, available at http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2509457, (2014).\n",
    "^ Jim Gatheral and Roel Oomen, Zero-intelligence realized variance estimation, *Finance and Stochastics* **14**(2) 249-283 (2010).\n",
    "^ Carl J. Nuzman and H. Vincent Poor, Linear estimation of self-similar processes via Lamperti’s transformation, *Journal of Applied Probability* **37**(2) 429-452 (2000)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
